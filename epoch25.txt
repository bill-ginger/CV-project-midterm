
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=8,
                                                 verbose=False, threshold= 1e-4, threshold_mode='rel', min_lr=1e-5)

0	0	loss:	 2.3443496227264404	lr: 0.01
0	50	loss:	 1.5519808530807495	lr: 0.01
0	100	loss:	 1.5255261659622192	lr: 0.01
0	150	loss:	 1.3009711503982544	lr: 0.01
0	200	loss:	 1.3276090621948242	lr: 0.01
0	250	loss:	 1.1319080591201782	lr: 0.01
0	300	loss:	 0.9903351068496704	lr: 0.01
0	350	loss:	 1.1708558797836304	lr: 0.01
epoch: 0	train acc: 0.51794
epoch: 0	test acc: 0.6116
1	0	loss:	 0.8456229567527771	lr: 0.01
1	50	loss:	 1.00739324092865	lr: 0.01
1	100	loss:	 0.7609418034553528	lr: 0.01
1	150	loss:	 0.7399674654006958	lr: 0.01
1	200	loss:	 0.980728268623352	lr: 0.01
1	250	loss:	 0.8247590661048889	lr: 0.01
1	300	loss:	 0.6073653697967529	lr: 0.01
1	350	loss:	 0.7635326385498047	lr: 0.01
epoch: 1	train acc: 0.72026
epoch: 1	test acc: 0.7367
2	0	loss:	 0.5158226490020752	lr: 0.01
2	50	loss:	 0.5531680583953857	lr: 0.01
2	100	loss:	 0.5844438076019287	lr: 0.01
2	150	loss:	 0.5547505617141724	lr: 0.01
2	200	loss:	 0.5993877649307251	lr: 0.01
2	250	loss:	 0.6170706748962402	lr: 0.01
2	300	loss:	 0.44409579038619995	lr: 0.01
2	350	loss:	 0.5174834728240967	lr: 0.01
epoch: 2	train acc: 0.80752
epoch: 2	test acc: 0.7062
3	0	loss:	 0.3501686751842499	lr: 0.01
3	50	loss:	 0.3539595305919647	lr: 0.01
3	100	loss:	 0.3768969178199768	lr: 0.01
3	150	loss:	 0.3661487102508545	lr: 0.01
3	200	loss:	 0.4334394335746765	lr: 0.01
3	250	loss:	 0.4370189905166626	lr: 0.01
3	300	loss:	 0.3415370583534241	lr: 0.01
3	350	loss:	 0.3564455211162567	lr: 0.01
epoch: 3	train acc: 0.8638
epoch: 3	test acc: 0.71
4	0	loss:	 0.2756066918373108	lr: 0.01
4	50	loss:	 0.43959933519363403	lr: 0.01
4	100	loss:	 0.27166548371315	lr: 0.01
4	150	loss:	 0.32191887497901917	lr: 0.01
4	200	loss:	 0.3264540731906891	lr: 0.01
4	250	loss:	 0.3044913411140442	lr: 0.01
4	300	loss:	 0.2702690660953522	lr: 0.01
4	350	loss:	 0.26259931921958923	lr: 0.01
epoch: 4	train acc: 0.89356
epoch: 4	test acc: 0.704
5	0	loss:	 0.2326921969652176	lr: 0.01
5	50	loss:	 0.23114606738090515	lr: 0.01
5	100	loss:	 0.11747041344642639	lr: 0.01
5	150	loss:	 0.27544254064559937	lr: 0.01
5	200	loss:	 0.3137509822845459	lr: 0.01
5	250	loss:	 0.27327823638916016	lr: 0.01
5	300	loss:	 0.2896963953971863	lr: 0.01
5	350	loss:	 0.21148096024990082	lr: 0.01
epoch: 5	train acc: 0.9227
epoch: 5	test acc: 0.7363
6	0	loss:	 0.18787196278572083	lr: 0.01
6	50	loss:	 0.16510166227817535	lr: 0.01
6	100	loss:	 0.19661666452884674	lr: 0.01
6	150	loss:	 0.1169288158416748	lr: 0.01
6	200	loss:	 0.15090405941009521	lr: 0.01
6	250	loss:	 0.14986540377140045	lr: 0.01
6	300	loss:	 0.09170759469270706	lr: 0.01
6	350	loss:	 0.1497436910867691	lr: 0.01
epoch: 6	train acc: 0.94514
epoch: 6	test acc: 0.722
7	0	loss:	 0.13675709068775177	lr: 0.01
7	50	loss:	 0.17140673100948334	lr: 0.01
7	100	loss:	 0.07372051477432251	lr: 0.01
7	150	loss:	 0.17523671686649323	lr: 0.01
7	200	loss:	 0.09943608939647675	lr: 0.01
7	250	loss:	 0.11305241286754608	lr: 0.01
7	300	loss:	 0.15961554646492004	lr: 0.01
7	350	loss:	 0.11313960701227188	lr: 0.01
epoch: 7	train acc: 0.95906
epoch: 7	test acc: 0.7735
8	0	loss:	 0.0952809527516365	lr: 0.01
8	50	loss:	 0.08715319633483887	lr: 0.01
8	100	loss:	 0.11089613288640976	lr: 0.01
8	150	loss:	 0.05644813925027847	lr: 0.01
8	200	loss:	 0.10393965989351273	lr: 0.01
8	250	loss:	 0.11124060302972794	lr: 0.01
8	300	loss:	 0.06765156239271164	lr: 0.01
8	350	loss:	 0.0673099011182785	lr: 0.01
epoch: 8	train acc: 0.97432
epoch: 8	test acc: 0.796
9	0	loss:	 0.024581758305430412	lr: 0.01
9	50	loss:	 0.058818966150283813	lr: 0.01
9	100	loss:	 0.04871611297130585	lr: 0.01
9	150	loss:	 0.08231333643198013	lr: 0.01
9	200	loss:	 0.08925074338912964	lr: 0.01
9	250	loss:	 0.05087039992213249	lr: 0.01
9	300	loss:	 0.0531308613717556	lr: 0.01
9	350	loss:	 0.054661646485328674	lr: 0.01
epoch: 9	train acc: 0.98408
epoch: 9	test acc: 0.7951
10	0	loss:	 0.03537094593048096	lr: 0.01
10	50	loss:	 0.04330386593937874	lr: 0.01
10	100	loss:	 0.030498778447508812	lr: 0.01
10	150	loss:	 0.03148570656776428	lr: 0.01
10	200	loss:	 0.0675063505768776	lr: 0.01
10	250	loss:	 0.02120591700077057	lr: 0.01
10	300	loss:	 0.010333478450775146	lr: 0.01
10	350	loss:	 0.018758244812488556	lr: 0.01
epoch: 10	train acc: 0.98902
epoch: 10	test acc: 0.8
11	0	loss:	 0.042345982044935226	lr: 0.01
11	50	loss:	 0.007848565466701984	lr: 0.01
11	100	loss:	 0.031428564339876175	lr: 0.01
11	150	loss:	 0.014690995216369629	lr: 0.01
11	200	loss:	 0.02949049510061741	lr: 0.01
11	250	loss:	 0.028961820527911186	lr: 0.01
11	300	loss:	 0.024365440011024475	lr: 0.01
11	350	loss:	 0.014975697733461857	lr: 0.01
epoch: 11	train acc: 0.99216
epoch: 11	test acc: 0.8062
12	0	loss:	 0.010759026743471622	lr: 0.01
12	50	loss:	 0.02365003153681755	lr: 0.01
12	100	loss:	 0.03268872946500778	lr: 0.01
12	150	loss:	 0.004552251193672419	lr: 0.01
12	200	loss:	 0.043791718780994415	lr: 0.01
12	250	loss:	 0.035887300968170166	lr: 0.01
12	300	loss:	 0.012810952961444855	lr: 0.01
12	350	loss:	 0.013619886711239815	lr: 0.01
epoch: 12	train acc: 0.99524
epoch: 12	test acc: 0.813
13	0	loss:	 0.016634302213788033	lr: 0.01
13	50	loss:	 0.013915805146098137	lr: 0.01
13	100	loss:	 0.004546082578599453	lr: 0.01
13	150	loss:	 0.015141304582357407	lr: 0.01
13	200	loss:	 0.006715642753988504	lr: 0.01
13	250	loss:	 0.005008867010474205	lr: 0.01
13	300	loss:	 0.004390995483845472	lr: 0.01
13	350	loss:	 0.01251379307359457	lr: 0.01
epoch: 13	train acc: 0.99808
epoch: 13	test acc: 0.8247
14	0	loss:	 0.0023129982873797417	lr: 0.01
14	50	loss:	 0.00675563607364893	lr: 0.01
14	100	loss:	 0.002555127488449216	lr: 0.01
14	150	loss:	 0.0022418678272515535	lr: 0.01
14	200	loss:	 0.0049179974012076855	lr: 0.01
14	250	loss:	 0.0020463746041059494	lr: 0.01
14	300	loss:	 0.0027224374935030937	lr: 0.01
14	350	loss:	 0.0020313593558967113	lr: 0.01
epoch: 14	train acc: 0.99968
epoch: 14	test acc: 0.8429
15	0	loss:	 0.0018699723295867443	lr: 0.01
15	50	loss:	 0.001559055526740849	lr: 0.01
15	100	loss:	 0.0015676553593948483	lr: 0.01
15	150	loss:	 0.0009922254830598831	lr: 0.01
15	200	loss:	 0.0007784054614603519	lr: 0.01
15	250	loss:	 0.0011878202203661203	lr: 0.01
15	300	loss:	 0.0009346880833618343	lr: 0.01
15	350	loss:	 0.0008703325293026865	lr: 0.01
epoch: 15	train acc: 1.0
epoch: 15	test acc: 0.8486
16	0	loss:	 0.0009199305204674602	lr: 0.01
16	50	loss:	 0.0013150933664292097	lr: 0.01
16	100	loss:	 0.0011471739271655679	lr: 0.01
16	150	loss:	 0.0010083658853545785	lr: 0.01
16	200	loss:	 0.0009859795682132244	lr: 0.01
16	250	loss:	 0.001232542679645121	lr: 0.01
16	300	loss:	 0.0010095835896208882	lr: 0.01
16	350	loss:	 0.0009067552164196968	lr: 0.01
epoch: 16	train acc: 1.0
epoch: 16	test acc: 0.8498
17	0	loss:	 0.0009497712599113584	lr: 0.01
17	50	loss:	 0.0013714025262743235	lr: 0.01
17	100	loss:	 0.0011841356754302979	lr: 0.01
17	150	loss:	 0.0011423812247812748	lr: 0.01
17	200	loss:	 0.0012080271262675524	lr: 0.01
17	250	loss:	 0.0013518495252355933	lr: 0.01
17	300	loss:	 0.0011432258179411292	lr: 0.01
17	350	loss:	 0.0010726329637691379	lr: 0.01
epoch: 17	train acc: 1.0
epoch: 17	test acc: 0.8504
18	0	loss:	 0.0010984176769852638	lr: 0.01
18	50	loss:	 0.0015054232208058238	lr: 0.01
18	100	loss:	 0.001311945728957653	lr: 0.01
18	150	loss:	 0.0013172331964597106	lr: 0.01
18	200	loss:	 0.001428370364010334	lr: 0.01
18	250	loss:	 0.0014907654840499163	lr: 0.01
18	300	loss:	 0.0012768403394147754	lr: 0.01
18	350	loss:	 0.001255962299183011	lr: 0.01
epoch: 18	train acc: 1.0
epoch: 18	test acc: 0.8512
19	0	loss:	 0.0012493233662098646	lr: 0.01
19	50	loss:	 0.0016373685793951154	lr: 0.01
19	100	loss:	 0.0014440553495660424	lr: 0.01
19	150	loss:	 0.0014825015095993876	lr: 0.01
19	200	loss:	 0.0016181747196242213	lr: 0.01
19	250	loss:	 0.0016244821017608047	lr: 0.01
19	300	loss:	 0.0013962595257908106	lr: 0.01
19	350	loss:	 0.0014262028271332383	lr: 0.01
epoch: 19	train acc: 1.0
epoch: 19	test acc: 0.8522
20	0	loss:	 0.0013807930517941713	lr: 0.01
20	50	loss:	 0.0017507715383544564	lr: 0.01
20	100	loss:	 0.0015617028111591935	lr: 0.01
20	150	loss:	 0.0016180628444999456	lr: 0.01
20	200	loss:	 0.0017758392496034503	lr: 0.01
20	250	loss:	 0.0017435686895623803	lr: 0.01
20	300	loss:	 0.0015043732710182667	lr: 0.01
20	350	loss:	 0.0015719715738669038	lr: 0.01
epoch: 20	train acc: 1.0
epoch: 20	test acc: 0.8531
21	0	loss:	 0.0014941276749596	lr: 0.01
21	50	loss:	 0.001839664881117642	lr: 0.01
21	100	loss:	 0.0016561688389629126	lr: 0.01
21	150	loss:	 0.0017257153522223234	lr: 0.01
21	200	loss:	 0.00190205208491534	lr: 0.01
21	250	loss:	 0.0018402579007670283	lr: 0.01
21	300	loss:	 0.0016021819319576025	lr: 0.01
21	350	loss:	 0.0016845205100253224	lr: 0.01
epoch: 21	train acc: 1.0
epoch: 21	test acc: 0.8536
22	0	loss:	 0.0015874920645728707	lr: 0.01
22	50	loss:	 0.0018982066540047526	lr: 0.01
22	100	loss:	 0.0017225209157913923	lr: 0.01
22	150	loss:	 0.0018008096376433969	lr: 0.01
22	200	loss:	 0.0019946759566664696	lr: 0.01
22	250	loss:	 0.001918837078846991	lr: 0.01
22	300	loss:	 0.0016848808154463768	lr: 0.01
22	350	loss:	 0.001766029978170991	lr: 0.01
epoch: 22	train acc: 1.0
epoch: 22	test acc: 0.8537
23	0	loss:	 0.001660871785134077	lr: 0.01
23	50	loss:	 0.0019309241324663162	lr: 0.01
23	100	loss:	 0.0017670391825959086	lr: 0.01
23	150	loss:	 0.0018510052468627691	lr: 0.01