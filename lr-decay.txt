import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import numpy as np
import random
import datetime,time
import matplotlib.pyplot as plt
from resnet18 import ResNet18


# 确保模型可复现
def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
setup_seed(6666)


def load_data(batch_size=128):
    data_root = '../yyk_pj/data'
#     transform = transforms.Compose(
#         [transforms.ToTensor(),
#          transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    train_set = torchvision.datasets.CIFAR10(root=data_root, train=True, download=False, transform=transform_train)
    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=16)
    test_set = torchvision.datasets.CIFAR10(root=data_root, train=False, download=False, transform=transform_test)
    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=16)
    return train_loader, test_loader


def train(epoch):
    model.train()
    total_correct = 0
    total_num = 0
    total_loss = 0
    for batch_idx, (x, label) in enumerate(train_loader):
        x, label = x.to(device), label.to(device)
        pred = model(x)
        loss = criterion(pred, label)
        all_loss.append(loss)
        total_loss += loss.item()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % 50 == 0:
            scheduler.step(total_loss)
            print(f'{epoch}\t{batch_idx}\tloss:\t {loss.item()}\tlr: {optimizer.param_groups[0]["lr"]:.5f}')
            total_loss = 0

        correct = torch.eq(pred.argmax(dim=1), label).float().sum().item()
        total_correct += correct
        total_num += x.shape[0]
    acc = total_correct / total_num
    train_acc.append(acc)
    print(f'epoch: {epoch}\ttrain acc: {acc}')


def test(epoch):
    model.eval()
    total_correct = 0
    total_num = 0
    with torch.no_grad():
        for x, label in test_loader:
            x, label = x.to(device), label.to(device)
            logits = model(x)
            pred = logits.argmax(dim=1)
            correct = torch.eq(pred, label).float().sum().item()
            total_correct += correct
            total_num += x.size(0)

        acc = total_correct / total_num
        test_acc.append(acc)
    print(f'epoch: {epoch}\ttest acc: {acc}')
    return acc


train_loader, test_loader = load_data()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ResNet18().to(device)
optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0005)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=20,
                                                 verbose=False, threshold= 1e-3, threshold_mode='rel', min_lr=1e-4)













0	0	loss:	 2.336637496948242	lr: 0.10000
0	50	loss:	 2.27158260345459	lr: 0.10000
0	100	loss:	 1.9862200021743774	lr: 0.10000
0	150	loss:	 1.711639404296875	lr: 0.10000
0	200	loss:	 1.7620128393173218	lr: 0.10000
0	250	loss:	 1.935809850692749	lr: 0.10000
0	300	loss:	 1.5484462976455688	lr: 0.10000
0	350	loss:	 1.4710967540740967	lr: 0.10000
epoch: 0	train acc: 0.29424
epoch: 0	test acc: 0.393
1	0	loss:	 1.6021275520324707	lr: 0.10000
1	50	loss:	 1.6135694980621338	lr: 0.10000
1	100	loss:	 1.6308034658432007	lr: 0.10000
1	150	loss:	 1.5647051334381104	lr: 0.10000
1	200	loss:	 1.3778877258300781	lr: 0.10000
1	250	loss:	 1.4964765310287476	lr: 0.10000
1	300	loss:	 1.4010611772537231	lr: 0.10000
1	350	loss:	 1.6340100765228271	lr: 0.10000
epoch: 1	train acc: 0.44386
epoch: 1	test acc: 0.5091
2	0	loss:	 1.470592737197876	lr: 0.10000
2	50	loss:	 1.3015589714050293	lr: 0.10000
2	100	loss:	 1.1827534437179565	lr: 0.10000
2	150	loss:	 1.288815975189209	lr: 0.10000
2	200	loss:	 1.3150135278701782	lr: 0.10000
2	250	loss:	 1.2094223499298096	lr: 0.10000
2	300	loss:	 1.2158381938934326	lr: 0.10000
2	350	loss:	 1.2217341661453247	lr: 0.10000
epoch: 2	train acc: 0.53546
epoch: 2	test acc: 0.5439
3	0	loss:	 1.168992280960083	lr: 0.10000
3	50	loss:	 1.0312763452529907	lr: 0.10000
3	100	loss:	 1.0097793340682983	lr: 0.10000
3	150	loss:	 1.0464953184127808	lr: 0.10000
3	200	loss:	 1.0833027362823486	lr: 0.10000
3	250	loss:	 0.9935334920883179	lr: 0.10000
3	300	loss:	 1.1419302225112915	lr: 0.10000
3	350	loss:	 0.9921451210975647	lr: 0.10000
epoch: 3	train acc: 0.61678
epoch: 3	test acc: 0.6143
4	0	loss:	 0.9614021182060242	lr: 0.10000
4	50	loss:	 0.9460517764091492	lr: 0.10000
4	100	loss:	 0.8177356719970703	lr: 0.10000
4	150	loss:	 0.9080861210823059	lr: 0.10000
4	200	loss:	 0.8970152139663696	lr: 0.10000
4	250	loss:	 0.9556635022163391	lr: 0.10000
4	300	loss:	 0.7861409187316895	lr: 0.10000
4	350	loss:	 0.9474083185195923	lr: 0.10000
epoch: 4	train acc: 0.67386
epoch: 4	test acc: 0.6951
5	0	loss:	 0.8641132116317749	lr: 0.10000
5	50	loss:	 0.8046188354492188	lr: 0.10000
5	100	loss:	 0.8759148716926575	lr: 0.10000
5	150	loss:	 0.8754940629005432	lr: 0.10000
5	200	loss:	 0.7853108644485474	lr: 0.10000
5	250	loss:	 0.775493860244751	lr: 0.10000
5	300	loss:	 0.6866821646690369	lr: 0.10000
5	350	loss:	 0.6115732789039612	lr: 0.10000
epoch: 5	train acc: 0.72372
epoch: 5	test acc: 0.7276
6	0	loss:	 0.7195195555686951	lr: 0.10000
6	50	loss:	 0.655025064945221	lr: 0.10000
6	100	loss:	 0.7023259997367859	lr: 0.10000
6	150	loss:	 0.7303131818771362	lr: 0.10000
6	200	loss:	 0.6708498597145081	lr: 0.10000
6	250	loss:	 0.7652945518493652	lr: 0.10000
6	300	loss:	 0.6293885707855225	lr: 0.10000
6	350	loss:	 0.47058358788490295	lr: 0.10000
epoch: 6	train acc: 0.7612
epoch: 6	test acc: 0.7257
7	0	loss:	 0.5695198774337769	lr: 0.10000
7	50	loss:	 0.7065955400466919	lr: 0.10000
7	100	loss:	 0.5219351649284363	lr: 0.10000
7	150	loss:	 0.6169195771217346	lr: 0.10000
7	200	loss:	 0.5143739581108093	lr: 0.10000
7	250	loss:	 0.6309232711791992	lr: 0.10000
7	300	loss:	 0.717367947101593	lr: 0.10000
7	350	loss:	 0.4865546226501465	lr: 0.10000
epoch: 7	train acc: 0.7814
epoch: 7	test acc: 0.7679
8	0	loss:	 0.47943440079689026	lr: 0.10000
8	50	loss:	 0.5465413928031921	lr: 0.10000
8	100	loss:	 0.5287737846374512	lr: 0.10000
8	150	loss:	 0.5491620302200317	lr: 0.10000
8	200	loss:	 0.43229126930236816	lr: 0.10000
8	250	loss:	 0.46937111020088196	lr: 0.10000
8	300	loss:	 0.6988062858581543	lr: 0.10000
8	350	loss:	 0.5460442304611206	lr: 0.10000
epoch: 8	train acc: 0.79792
epoch: 8	test acc: 0.8033
9	0	loss:	 0.4738254249095917	lr: 0.10000
9	50	loss:	 0.6763656735420227	lr: 0.10000
9	100	loss:	 0.462847501039505	lr: 0.10000
9	150	loss:	 0.42727336287498474	lr: 0.10000
9	200	loss:	 0.6332967281341553	lr: 0.10000
9	250	loss:	 0.5294219255447388	lr: 0.10000
9	300	loss:	 0.470073938369751	lr: 0.10000
9	350	loss:	 0.591961681842804	lr: 0.10000
epoch: 9	train acc: 0.80954
epoch: 9	test acc: 0.783
10	0	loss:	 0.500505805015564	lr: 0.10000
10	50	loss:	 0.5318954586982727	lr: 0.10000
10	100	loss:	 0.4136163592338562	lr: 0.10000
10	150	loss:	 0.5156558752059937	lr: 0.10000
10	200	loss:	 0.5493537783622742	lr: 0.10000
10	250	loss:	 0.5086507201194763	lr: 0.10000
10	300	loss:	 0.44125044345855713	lr: 0.10000
10	350	loss:	 0.46707451343536377	lr: 0.10000
epoch: 10	train acc: 0.82068
epoch: 10	test acc: 0.7874
11	0	loss:	 0.4835859537124634	lr: 0.10000
11	50	loss:	 0.43791988492012024	lr: 0.10000
11	100	loss:	 0.5313018560409546	lr: 0.10000
11	150	loss:	 0.47479256987571716	lr: 0.10000
11	200	loss:	 0.447543203830719	lr: 0.10000
11	250	loss:	 0.4279543459415436	lr: 0.01000
11	300	loss:	 0.405539870262146	lr: 0.01000
11	350	loss:	 0.39123061299324036	lr: 0.01000
epoch: 11	train acc: 0.83996
epoch: 11	test acc: 0.8702
12	0	loss:	 0.2601202130317688	lr: 0.01000
12	50	loss:	 0.2509090304374695	lr: 0.01000
12	100	loss:	 0.29833728075027466	lr: 0.01000
12	150	loss:	 0.25927501916885376	lr: 0.01000
12	200	loss:	 0.3191027343273163	lr: 0.01000
12	250	loss:	 0.26297158002853394	lr: 0.01000
12	300	loss:	 0.262935996055603	lr: 0.01000
12	350	loss:	 0.3741006851196289	lr: 0.01000
epoch: 12	train acc: 0.89632
epoch: 12	test acc: 0.8844
13	0	loss:	 0.34578731656074524	lr: 0.01000
13	50	loss:	 0.2536758780479431	lr: 0.01000
13	100	loss:	 0.34646421670913696	lr: 0.01000
13	150	loss:	 0.34338870644569397	lr: 0.01000
13	200	loss:	 0.36755532026290894	lr: 0.01000
13	250	loss:	 0.3157893717288971	lr: 0.01000
13	300	loss:	 0.15922041237354279	lr: 0.01000
13	350	loss:	 0.3172098398208618	lr: 0.01000
epoch: 13	train acc: 0.90826
epoch: 13	test acc: 0.8927
14	0	loss:	 0.3100360929965973	lr: 0.01000
14	50	loss:	 0.22465263307094574	lr: 0.01000
14	100	loss:	 0.24504119157791138	lr: 0.01000
14	150	loss:	 0.320200651884079	lr: 0.01000
14	200	loss:	 0.2697308361530304	lr: 0.01000
14	250	loss:	 0.2165222018957138	lr: 0.00100
14	300	loss:	 0.2554950416088104	lr: 0.00100
14	350	loss:	 0.17214573919773102	lr: 0.00100
epoch: 14	train acc: 0.91462
epoch: 14	test acc: 0.8977
15	0	loss:	 0.1933668553829193	lr: 0.00100
15	50	loss:	 0.18980427086353302	lr: 0.00100
15	100	loss:	 0.20745693147182465	lr: 0.00100
15	150	loss:	 0.22140847146511078	lr: 0.00100
15	200	loss:	 0.35152000188827515	lr: 0.00100
15	250	loss:	 0.15835171937942505	lr: 0.00100
15	300	loss:	 0.4398066997528076	lr: 0.00100
15	350	loss:	 0.2448887974023819	lr: 0.00100
epoch: 15	train acc: 0.92668
epoch: 15	test acc: 0.8988
16	0	loss:	 0.2885148823261261	lr: 0.00100
16	50	loss:	 0.18327201902866364	lr: 0.00100
16	100	loss:	 0.20253127813339233	lr: 0.00100
16	150	loss:	 0.1864803433418274	lr: 0.00100
16	200	loss:	 0.2677662968635559	lr: 0.00100
16	250	loss:	 0.17654944956302643	lr: 0.00100
16	300	loss:	 0.2176411896944046	lr: 0.00100
16	350	loss:	 0.19103749096393585	lr: 0.00100
epoch: 16	train acc: 0.92838
epoch: 16	test acc: 0.899
17	0	loss:	 0.1460365653038025	lr: 0.00100
17	50	loss:	 0.1801721602678299	lr: 0.00100
17	100	loss:	 0.26274552941322327	lr: 0.00100
17	150	loss:	 0.22781063616275787	lr: 0.00100
17	200	loss:	 0.2597249448299408	lr: 0.00100
17	250	loss:	 0.17433612048625946	lr: 0.00100
17	300	loss:	 0.282399445772171	lr: 0.00100
17	350	loss:	 0.18639999628067017	lr: 0.00100
epoch: 17	train acc: 0.92934
epoch: 17	test acc: 0.9004
18	0	loss:	 0.22427783906459808	lr: 0.00100
18	50	loss:	 0.22728709876537323	lr: 0.00100
18	100	loss:	 0.41008490324020386	lr: 0.00100
18	150	loss:	 0.28906679153442383	lr: 0.00100
18	200	loss:	 0.18230584263801575	lr: 0.00100
18	250	loss:	 0.32713642716407776	lr: 0.00100
18	300	loss:	 0.19513510167598724	lr: 0.00100
18	350	loss:	 0.2610495090484619	lr: 0.00100
epoch: 18	train acc: 0.92904
epoch: 18	test acc: 0.9005
19	0	loss:	 0.1742405742406845	lr: 0.00100
19	50	loss:	 0.3235447406768799	lr: 0.00100
19	100	loss:	 0.10042297840118408	lr: 0.00100
19	150	loss:	 0.1484832465648651	lr: 0.00100
19	200	loss:	 0.29865965247154236	lr: 0.00100
19	250	loss:	 0.169851616024971	lr: 0.00010
19	300	loss:	 0.18610131740570068	lr: 0.00010
19	350	loss:	 0.2706858813762665	lr: 0.00010
epoch: 19	train acc: 0.92942
epoch: 19	test acc: 0.9012
20	0	loss:	 0.21177682280540466	lr: 0.00010
20	50	loss:	 0.08839379996061325	lr: 0.00010
20	100	loss:	 0.32095327973365784	lr: 0.00010
20	150	loss:	 0.1784125417470932	lr: 0.00010
20	200	loss:	 0.23563726246356964	lr: 0.00010
20	250	loss:	 0.21521925926208496	lr: 0.00010
20	300	loss:	 0.3258792459964752	lr: 0.00010
20	350	loss:	 0.23733298480510712	lr: 0.00010
epoch: 20	train acc: 0.93144
epoch: 20	test acc: 0.9006
21	0	loss:	 0.1430913209915161	lr: 0.00010
21	50	loss:	 0.22573834657669067	lr: 0.00010
21	100	loss:	 0.15095368027687073	lr: 0.00010
21	150	loss:	 0.14537981152534485	lr: 0.00010
21	200	loss:	 0.2995874285697937	lr: 0.00010
21	250	loss:	 0.17558397352695465	lr: 0.00010
21	300	loss:	 0.28434285521507263	lr: 0.00010
21	350	loss:	 0.23773948848247528	lr: 0.00010
epoch: 21	train acc: 0.93072
epoch: 21	test acc: 0.9012
22	0	loss:	 0.20166093111038208	lr: 0.00010
22	50	loss:	 0.15853814780712128	lr: 0.00010
22	100	loss:	 0.19263142347335815	lr: 0.00010
22	150	loss:	 0.165519118309021	lr: 0.00010
22	200	loss:	 0.2733706533908844	lr: 0.00010
22	250	loss:	 0.18144570291042328	lr: 0.00010
22	300	loss:	 0.23266203701496124	lr: 0.00010
22	350	loss:	 0.15153880417346954	lr: 0.00010
epoch: 22	train acc: 0.93188
epoch: 22	test acc: 0.9018
23	0	loss:	 0.12729057669639587	lr: 0.00010
23	50	loss:	 0.16564826667308807	lr: 0.00010
23	100	loss:	 0.15006954967975616	lr: 0.00010
23	150	loss:	 0.1469755321741104	lr: 0.00010
23	200	loss:	 0.10883773118257523	lr: 0.00010
23	250	loss:	 0.16063979268074036	lr: 0.00010
23	300	loss:	 0.15751765668392181	lr: 0.00010
23	350	loss:	 0.09800883382558823	lr: 0.00010
epoch: 23	train acc: 0.93278
epoch: 23	test acc: 0.9014
24	0	loss:	 0.25567153096199036	lr: 0.00010
24	50	loss:	 0.21611420810222626	lr: 0.00010
24	100	loss:	 0.1080942377448082	lr: 0.00010
24	150	loss:	 0.17346742749214172	lr: 0.00010
24	200	loss:	 0.16744551062583923	lr: 0.00010
24	250	loss:	 0.18723103404045105	lr: 0.00010
24	300	loss:	 0.1574128270149231	lr: 0.00010
24	350	loss:	 0.1413203775882721	lr: 0.00010
epoch: 24	train acc: 0.93398
epoch: 24	test acc: 0.901
25	0	loss:	 0.2856062948703766	lr: 0.00010
25	50	loss:	 0.27830228209495544	lr: 0.00010
25	100	loss:	 0.22953063249588013	lr: 0.00010
25	150	loss:	 0.15234977006912231	lr: 0.00010
25	200	loss:	 0.19468770921230316	lr: 0.00010
25	250	loss:	 0.16647987067699432	lr: 0.00010
25	300	loss:	 0.14686761796474457	lr: 0.00010
25	350	loss:	 0.2250703126192093	lr: 0.00010
epoch: 25	train acc: 0.93366
epoch: 25	test acc: 0.9009
26	0	loss:	 0.17036332190036774	lr: 0.00010
26	50	loss:	 0.24639730155467987	lr: 0.00010
26	100	loss:	 0.16728127002716064	lr: 0.00010
26	150	loss:	 0.2102319449186325	lr: 0.00010
26	200	loss:	 0.25944647192955017	lr: 0.00010
26	250	loss:	 0.21602439880371094	lr: 0.00010
26	300	loss:	 0.18275484442710876	lr: 0.00010
26	350	loss:	 0.2182757705450058	lr: 0.00010
epoch: 26	train acc: 0.93204
epoch: 26	test acc: 0.9009
27	0	loss:	 0.22601918876171112	lr: 0.00010
27	50	loss:	 0.21106305718421936	lr: 0.00010
27	100	loss:	 0.20108956098556519	lr: 0.00010
27	150	loss:	 0.1306537687778473	lr: 0.00010
27	200	loss:	 0.16495835781097412	lr: 0.00010
27	250	loss:	 0.16524909436702728	lr: 0.00010
27	300	loss:	 0.2124829888343811	lr: 0.00010
27	350	loss:	 0.1512317955493927	lr: 0.00010
epoch: 27	train acc: 0.93232
epoch: 27	test acc: 0.9008
28	0	loss:	 0.16192986071109772	lr: 0.00010
28	50	loss:	 0.14855468273162842	lr: 0.00010
28	100	loss:	 0.16787870228290558	lr: 0.00010
28	150	loss:	 0.23344150185585022	lr: 0.00010
28	200	loss:	 0.15763387084007263	lr: 0.00010
28	250	loss:	 0.18452826142311096	lr: 0.00010
28	300	loss:	 0.29809004068374634	lr: 0.00010
28	350	loss:	 0.30871450901031494	lr: 0.00010
epoch: 28	train acc: 0.93236
epoch: 28	test acc: 0.9015
29	0	loss:	 0.17882677912712097	lr: 0.00010
29	50	loss:	 0.19498682022094727	lr: 0.00010
29	100	loss:	 0.3058430254459381	lr: 0.00010
29	150	loss:	 0.1417030394077301	lr: 0.00010
29	200	loss:	 0.1969532072544098	lr: 0.00010
29	250	loss:	 0.1738707721233368	lr: 0.00010
29	300	loss:	 0.30755844712257385	lr: 0.00010
29	350	loss:	 0.25738272070884705	lr: 0.00010
epoch: 29	train acc: 0.93194
epoch: 29	test acc: 0.902
30	0	loss:	 0.30566614866256714	lr: 0.00010
30	50	loss:	 0.12533485889434814	lr: 0.00010
30	100	loss:	 0.1195274144411087	lr: 0.00010
30	150	loss:	 0.1273355633020401	lr: 0.00010
30	200	loss:	 0.2372623085975647	lr: 0.00010
30	250	loss:	 0.21658433973789215	lr: 0.00010
30	300	loss:	 0.1295633763074875	lr: 0.00010
30	350	loss:	 0.1478227823972702	lr: 0.00010
epoch: 30	train acc: 0.93304
epoch: 30	test acc: 0.9018
31	0	loss:	 0.21650250256061554	lr: 0.00010
31	50	loss:	 0.2523938715457916	lr: 0.00010
31	100	loss:	 0.13848893344402313	lr: 0.00010
31	150	loss:	 0.25278401374816895	lr: 0.00010
































0	0	loss:	 2.336637496948242	lr: 0.10000
0	50	loss:	 2.27158260345459	lr: 0.10000
0	100	loss:	 1.9862200021743774	lr: 0.10000
0	150	loss:	 1.711639404296875	lr: 0.10000
0	200	loss:	 1.7620128393173218	lr: 0.10000
0	250	loss:	 1.935809850692749	lr: 0.10000
0	300	loss:	 1.5484462976455688	lr: 0.10000
0	350	loss:	 1.4710967540740967	lr: 0.10000
epoch: 0	train acc: 0.29424
epoch: 0	test acc: 0.393
1	0	loss:	 1.6021275520324707	lr: 0.10000
1	50	loss:	 1.6135694980621338	lr: 0.10000
1	100	loss:	 1.6308034658432007	lr: 0.10000
1	150	loss:	 1.5647051334381104	lr: 0.10000
1	200	loss:	 1.3778877258300781	lr: 0.10000
1	250	loss:	 1.4964765310287476	lr: 0.10000
1	300	loss:	 1.4010611772537231	lr: 0.10000
1	350	loss:	 1.6340100765228271	lr: 0.10000
epoch: 1	train acc: 0.44386
epoch: 1	test acc: 0.5091
2	0	loss:	 1.470592737197876	lr: 0.10000
2	50	loss:	 1.3015589714050293	lr: 0.10000
2	100	loss:	 1.1827534437179565	lr: 0.10000
2	150	loss:	 1.288815975189209	lr: 0.10000
2	200	loss:	 1.3150135278701782	lr: 0.10000
2	250	loss:	 1.2094223499298096	lr: 0.10000
2	300	loss:	 1.2158381938934326	lr: 0.10000
2	350	loss:	 1.2217341661453247	lr: 0.10000
epoch: 2	train acc: 0.53546
epoch: 2	test acc: 0.5439
3	0	loss:	 1.168992280960083	lr: 0.10000
3	50	loss:	 1.0312763452529907	lr: 0.10000
3	100	loss:	 1.0097793340682983	lr: 0.10000
3	150	loss:	 1.0464953184127808	lr: 0.10000
3	200	loss:	 1.0833027362823486	lr: 0.10000
3	250	loss:	 0.9935334920883179	lr: 0.10000
3	300	loss:	 1.1419302225112915	lr: 0.10000
3	350	loss:	 0.9921451210975647	lr: 0.10000
epoch: 3	train acc: 0.61678
epoch: 3	test acc: 0.6143
4	0	loss:	 0.9614021182060242	lr: 0.10000
4	50	loss:	 0.9460517764091492	lr: 0.10000
4	100	loss:	 0.8177356719970703	lr: 0.10000
4	150	loss:	 0.9080861210823059	lr: 0.10000
4	200	loss:	 0.8970152139663696	lr: 0.10000
4	250	loss:	 0.9556635022163391	lr: 0.10000
4	300	loss:	 0.7861409187316895	lr: 0.10000
4	350	loss:	 0.9474083185195923	lr: 0.10000
epoch: 4	train acc: 0.67386
epoch: 4	test acc: 0.6951
5	0	loss:	 0.8641132116317749	lr: 0.10000
5	50	loss:	 0.8046188354492188	lr: 0.10000
5	100	loss:	 0.8759148716926575	lr: 0.10000
5	150	loss:	 0.8754940629005432	lr: 0.10000
5	200	loss:	 0.7853108644485474	lr: 0.10000
5	250	loss:	 0.775493860244751	lr: 0.10000
5	300	loss:	 0.6866821646690369	lr: 0.10000
5	350	loss:	 0.6115732789039612	lr: 0.10000
epoch: 5	train acc: 0.72372
epoch: 5	test acc: 0.7276
6	0	loss:	 0.7195195555686951	lr: 0.10000
6	50	loss:	 0.655025064945221	lr: 0.10000
6	100	loss:	 0.7023259997367859	lr: 0.10000
6	150	loss:	 0.7303131818771362	lr: 0.10000
6	200	loss:	 0.6708498597145081	lr: 0.10000
6	250	loss:	 0.7652945518493652	lr: 0.10000
6	300	loss:	 0.6293885707855225	lr: 0.10000
6	350	loss:	 0.47058358788490295	lr: 0.10000
epoch: 6	train acc: 0.7612
epoch: 6	test acc: 0.7257
7	0	loss:	 0.5695198774337769	lr: 0.10000
7	50	loss:	 0.7065955400466919	lr: 0.10000
7	100	loss:	 0.5219351649284363	lr: 0.10000
7	150	loss:	 0.6169195771217346	lr: 0.10000
7	200	loss:	 0.5143739581108093	lr: 0.10000
7	250	loss:	 0.6309232711791992	lr: 0.10000
7	300	loss:	 0.717367947101593	lr: 0.10000
7	350	loss:	 0.4865546226501465	lr: 0.10000
epoch: 7	train acc: 0.7814
epoch: 7	test acc: 0.7679
8	0	loss:	 0.47943440079689026	lr: 0.10000
8	50	loss:	 0.5465413928031921	lr: 0.10000
8	100	loss:	 0.5287737846374512	lr: 0.10000
8	150	loss:	 0.5491620302200317	lr: 0.10000
8	200	loss:	 0.43229126930236816	lr: 0.10000
8	250	loss:	 0.46937111020088196	lr: 0.10000
8	300	loss:	 0.6988062858581543	lr: 0.10000
8	350	loss:	 0.5460442304611206	lr: 0.10000
epoch: 8	train acc: 0.79792
epoch: 8	test acc: 0.8033
9	0	loss:	 0.4738254249095917	lr: 0.10000
9	50	loss:	 0.6763656735420227	lr: 0.10000
9	100	loss:	 0.462847501039505	lr: 0.10000
9	150	loss:	 0.42727336287498474	lr: 0.10000
9	200	loss:	 0.6332967281341553	lr: 0.10000
9	250	loss:	 0.5294219255447388	lr: 0.10000
9	300	loss:	 0.470073938369751	lr: 0.10000
9	350	loss:	 0.591961681842804	lr: 0.10000
epoch: 9	train acc: 0.80954
epoch: 9	test acc: 0.783
10	0	loss:	 0.500505805015564	lr: 0.01000
10	50	loss:	 0.5090621113777161	lr: 0.01000
10	100	loss:	 0.31123316287994385	lr: 0.01000
10	150	loss:	 0.3965461254119873	lr: 0.01000
10	200	loss:	 0.3233426511287689	lr: 0.01000
10	250	loss:	 0.2500741481781006	lr: 0.01000
10	300	loss:	 0.30214300751686096	lr: 0.01000
10	350	loss:	 0.2826678454875946	lr: 0.01000
epoch: 10	train acc: 0.87256
epoch: 10	test acc: 0.8702
11	0	loss:	 0.32745638489723206	lr: 0.01000
11	50	loss:	 0.3713264763355255	lr: 0.01000
11	100	loss:	 0.2983412742614746	lr: 0.01000
11	150	loss:	 0.309389591217041	lr: 0.01000
11	200	loss:	 0.32291659712791443	lr: 0.01000
11	250	loss:	 0.29181167483329773	lr: 0.01000
11	300	loss:	 0.300127774477005	lr: 0.01000
11	350	loss:	 0.32129156589508057	lr: 0.01000
epoch: 11	train acc: 0.8905
epoch: 11	test acc: 0.8772
12	0	loss:	 0.24317945539951324	lr: 0.01000
12	50	loss:	 0.21360616385936737	lr: 0.01000
12	100	loss:	 0.2470831573009491	lr: 0.01000
12	150	loss:	 0.21978329122066498	lr: 0.01000
12	200	loss:	 0.31676992774009705	lr: 0.01000
12	250	loss:	 0.30038008093833923	lr: 0.01000
12	300	loss:	 0.2187063843011856	lr: 0.01000
12	350	loss:	 0.3969506323337555	lr: 0.01000
epoch: 12	train acc: 0.90106
epoch: 12	test acc: 0.881
13	0	loss:	 0.3866877257823944	lr: 0.01000
13	50	loss:	 0.2502768933773041	lr: 0.01000
13	100	loss:	 0.2999703288078308	lr: 0.01000
13	150	loss:	 0.4019813537597656	lr: 0.01000
13	200	loss:	 0.3682273328304291	lr: 0.01000
13	250	loss:	 0.28514203429222107	lr: 0.01000
13	300	loss:	 0.16060620546340942	lr: 0.01000
13	350	loss:	 0.28687503933906555	lr: 0.01000
epoch: 13	train acc: 0.90794
epoch: 13	test acc: 0.8889
14	0	loss:	 0.2772234082221985	lr: 0.01000
14	50	loss:	 0.20981279015541077	lr: 0.01000
14	100	loss:	 0.2585561275482178	lr: 0.01000
14	150	loss:	 0.31712278723716736	lr: 0.01000
14	200	loss:	 0.24519242346286774	lr: 0.01000
14	250	loss:	 0.19843780994415283	lr: 0.01000
14	300	loss:	 0.2577590048313141	lr: 0.01000
14	350	loss:	 0.24322079122066498	lr: 0.01000
epoch: 14	train acc: 0.91564
epoch: 14	test acc: 0.8862
15	0	loss:	 0.21553266048431396	lr: 0.01000
15	50	loss:	 0.2038903385400772	lr: 0.01000
15	100	loss:	 0.25499212741851807	lr: 0.01000
15	150	loss:	 0.22515171766281128	lr: 0.01000
15	200	loss:	 0.3943144381046295	lr: 0.01000
15	250	loss:	 0.15822835266590118	lr: 0.01000
15	300	loss:	 0.4029821455478668	lr: 0.01000
15	350	loss:	 0.3187101185321808	lr: 0.01000
epoch: 15	train acc: 0.92116
epoch: 15	test acc: 0.8864
16	0	loss:	 0.24828658998012543	lr: 0.01000
16	50	loss:	 0.1730584055185318	lr: 0.01000
16	100	loss:	 0.25064823031425476	lr: 0.01000
16	150	loss:	 0.19306804239749908	lr: 0.01000
16	200	loss:	 0.31043702363967896	lr: 0.01000
16	250	loss:	 0.15660762786865234	lr: 0.01000
16	300	loss:	 0.16825728118419647	lr: 0.01000
16	350	loss:	 0.2079239934682846	lr: 0.01000
epoch: 16	train acc: 0.92412
epoch: 16	test acc: 0.888
17	0	loss:	 0.16612330079078674	lr: 0.01000
17	50	loss:	 0.23381704092025757	lr: 0.01000
17	100	loss:	 0.18378640711307526	lr: 0.01000
17	150	loss:	 0.2512994110584259	lr: 0.01000
17	200	loss:	 0.2528705596923828	lr: 0.01000
17	250	loss:	 0.19470076262950897	lr: 0.01000
17	300	loss:	 0.2946239113807678	lr: 0.01000
17	350	loss:	 0.18634265661239624	lr: 0.01000
epoch: 17	train acc: 0.9304
epoch: 17	test acc: 0.8916
18	0	loss:	 0.20941826701164246	lr: 0.01000
18	50	loss:	 0.1814221292734146	lr: 0.01000
18	100	loss:	 0.3660871088504791	lr: 0.01000
18	150	loss:	 0.26881974935531616	lr: 0.01000
18	200	loss:	 0.16124960780143738	lr: 0.01000
18	250	loss:	 0.25794345140457153	lr: 0.01000
18	300	loss:	 0.204077810049057	lr: 0.01000
18	350	loss:	 0.29217350482940674	lr: 0.01000
epoch: 18	train acc: 0.93118
epoch: 18	test acc: 0.8951
19	0	loss:	 0.16719576716423035	lr: 0.00100
19	50	loss:	 0.24470551311969757	lr: 0.00100
19	100	loss:	 0.06425327062606812	lr: 0.00100
19	150	loss:	 0.08716819435358047	lr: 0.00100
19	200	loss:	 0.23214319348335266	lr: 0.00100
19	250	loss:	 0.07956340909004211	lr: 0.00100
19	300	loss:	 0.10350844264030457	lr: 0.00100
19	350	loss:	 0.2134924829006195	lr: 0.00100
epoch: 19	train acc: 0.95004
epoch: 19	test acc: 0.9052
20	0	loss:	 0.16419437527656555	lr: 0.00100
20	50	loss:	 0.06804727017879486	lr: 0.00100
20	100	loss:	 0.23912136256694794	lr: 0.00100
20	150	loss:	 0.1452806293964386	lr: 0.00100
20	200	loss:	 0.16737449169158936	lr: 0.00100
20	250	loss:	 0.15333545207977295	lr: 0.00100
20	300	loss:	 0.24351149797439575	lr: 0.00100
20	350	loss:	 0.16336950659751892	lr: 0.00100
epoch: 20	train acc: 0.95446
epoch: 20	test acc: 0.9054
21	0	loss:	 0.11201060563325882	lr: 0.00100
21	50	loss:	 0.12656278908252716	lr: 0.00100
21	100	loss:	 0.10367102921009064	lr: 0.00100
21	150	loss:	 0.10245417803525925	lr: 0.00100
21	200	loss:	 0.22516164183616638	lr: 0.00100
21	250	loss:	 0.15001016855239868	lr: 0.00100
21	300	loss:	 0.16977494955062866	lr: 0.00100
21	350	loss:	 0.13293035328388214	lr: 0.00100
epoch: 21	train acc: 0.95672
epoch: 21	test acc: 0.9061
22	0	loss:	 0.13773664832115173	lr: 0.00100
22	50	loss:	 0.09423455595970154	lr: 0.00100
22	100	loss:	 0.10546308755874634	lr: 0.00100
22	150	loss:	 0.10728570073843002	lr: 0.00100
22	200	loss:	 0.20064711570739746	lr: 0.00100
22	250	loss:	 0.1547350287437439	lr: 0.00100
22	300	loss:	 0.1590004861354828	lr: 0.00100
22	350	loss:	 0.12447980791330338	lr: 0.00100
epoch: 22	train acc: 0.95906
epoch: 22	test acc: 0.9059
23	0	loss:	 0.08655314892530441	lr: 0.00100
23	50	loss:	 0.13002681732177734	lr: 0.00100
23	100	loss:	 0.08385937660932541	lr: 0.00100
23	150	loss:	 0.09585051238536835	lr: 0.00100
23	200	loss:	 0.060102302581071854	lr: 0.00100
23	250	loss:	 0.09621759504079819	lr: 0.00100
23	300	loss:	 0.0589115209877491	lr: 0.00100
23	350	loss:	 0.09221464395523071	lr: 0.00100
epoch: 23	train acc: 0.95908
epoch: 23	test acc: 0.9056
24	0	loss:	 0.16855952143669128	lr: 0.00100
24	50	loss:	 0.09877881407737732	lr: 0.00100
24	100	loss:	 0.08929865807294846	lr: 0.00100
24	150	loss:	 0.08159995824098587	lr: 0.00100
24	200	loss:	 0.1337573528289795	lr: 0.00100
24	250	loss:	 0.11426376551389694	lr: 0.00100
24	300	loss:	 0.09013194590806961	lr: 0.00100
24	350	loss:	 0.07050096988677979	lr: 0.00100
epoch: 24	train acc: 0.96272
epoch: 24	test acc: 0.9065
25	0	loss:	 0.1600324958562851	lr: 0.00100
25	50	loss:	 0.12910644710063934	lr: 0.00100
25	100	loss:	 0.13120482861995697	lr: 0.00100
25	150	loss:	 0.10374648869037628	lr: 0.00100
25	200	loss:	 0.11882378906011581	lr: 0.00100
25	250	loss:	 0.1138230562210083	lr: 0.00100
25	300	loss:	 0.08195314556360245	lr: 0.00100
25	350	loss:	 0.11392152309417725	lr: 0.00100
epoch: 25	train acc: 0.96158
epoch: 25	test acc: 0.9048
26	0	loss:	 0.08525148779153824	lr: 0.00100
26	50	loss:	 0.1678062379360199	lr: 0.00100
26	100	loss:	 0.06657735258340836	lr: 0.00100
26	150	loss:	 0.07520200312137604	lr: 0.00100
26	200	loss:	 0.08458339422941208	lr: 0.00100
26	250	loss:	 0.12021823972463608	lr: 0.00100
26	300	loss:	 0.10047350078821182	lr: 0.00100
26	350	loss:	 0.15083922445774078	lr: 0.00100
epoch: 26	train acc: 0.96422
epoch: 26	test acc: 0.9063
27	0	loss:	 0.14156539738178253	lr: 0.00100
27	50	loss:	 0.10955962538719177	lr: 0.00100
27	100	loss:	 0.09839843958616257	lr: 0.00100
27	150	loss:	 0.12061357498168945	lr: 0.00100
27	200	loss:	 0.10502233356237411	lr: 0.00100
27	250	loss:	 0.09166977554559708	lr: 0.00100
27	300	loss:	 0.11536658555269241	lr: 0.00100
27	350	loss:	 0.0896357074379921	lr: 0.00100
epoch: 27	train acc: 0.96506
epoch: 27	test acc: 0.9074
28	0	loss:	 0.0838397815823555	lr: 0.00010
28	50	loss:	 0.0741480141878128	lr: 0.00010
28	100	loss:	 0.08557593077421188	lr: 0.00010
28	150	loss:	 0.12438566982746124	lr: 0.00010
28	200	loss:	 0.06994671374559402	lr: 0.00010
28	250	loss:	 0.08430249989032745	lr: 0.00010
28	300	loss:	 0.14419887959957123	lr: 0.00010
28	350	loss:	 0.15997378528118134	lr: 0.00010
epoch: 28	train acc: 0.96692
epoch: 28	test acc: 0.9071
29	0	loss:	 0.11406523734331131	lr: 0.00010
29	50	loss:	 0.06899715960025787	lr: 0.00010
29	100	loss:	 0.17689862847328186	lr: 0.00010
29	150	loss:	 0.09800473600625992	lr: 0.00010
29	200	loss:	 0.09483461827039719	lr: 0.00010
29	250	loss:	 0.05760069936513901	lr: 0.00010
29	300	loss:	 0.190929114818573	lr: 0.00010
29	350	loss:	 0.10563520342111588	lr: 0.00010
epoch: 29	train acc: 0.96744
epoch: 29	test acc: 0.9072
30	0	loss:	 0.17025963962078094	lr: 0.00010
30	50	loss:	 0.04400826618075371	lr: 0.00010
30	100	loss:	 0.054320305585861206	lr: 0.00010
30	150	loss:	 0.061971284449100494	lr: 0.00010
30	200	loss:	 0.16589996218681335	lr: 0.00010
30	250	loss:	 0.1031411662697792	lr: 0.00010
30	300	loss:	 0.08620423078536987	lr: 0.00010
30	350	loss:	 0.05585506185889244	lr: 0.00010
epoch: 30	train acc: 0.96762
epoch: 30	test acc: 0.908
31	0	loss:	 0.14093342423439026	lr: 0.00010
31	50	loss:	 0.12107165902853012	lr: 0.00010
31	100	loss:	 0.06460738182067871	lr: 0.00010
31	150	loss:	 0.14888301491737366	lr: 0.00010
31	200	loss:	 0.0714062973856926	lr: 0.00010
31	250	loss:	 0.10879648476839066	lr: 0.00010
31	300	loss:	 0.10139951854944229	lr: 0.00010
31	350	loss:	 0.05429399386048317	lr: 0.00010
epoch: 31	train acc: 0.96856
epoch: 31	test acc: 0.9086
32	0	loss:	 0.10067519545555115	lr: 0.00010
32	50	loss:	 0.06918004900217056	lr: 0.00010
32	100	loss:	 0.07251084595918655	lr: 0.00010
32	150	loss:	 0.15056777000427246	lr: 0.00010
32	200	loss:	 0.04862368479371071	lr: 0.00010
32	250	loss:	 0.07087159156799316	lr: 0.00010
32	300	loss:	 0.139999657869339	lr: 0.00010
32	350	loss:	 0.062190841883420944	lr: 0.00010
epoch: 32	train acc: 0.9688
epoch: 32	test acc: 0.9088
33	0	loss:	 0.11811288446187973	lr: 0.00010
33	50	loss:	 0.1211819276213646	lr: 0.00010
33	100	loss:	 0.10006033629179001	lr: 0.00010
33	150	loss:	 0.0700356736779213	lr: 0.00010
33	200	loss:	 0.12710338830947876	lr: 0.00010
33	250	loss:	 0.06643776595592499	lr: 0.00010
33	300	loss:	 0.0856262668967247	lr: 0.00010
33	350	loss:	 0.06313826888799667	lr: 0.00010
epoch: 33	train acc: 0.96864
epoch: 33	test acc: 0.908
34	0	loss:	 0.07182680070400238	lr: 0.00010
34	50	loss:	 0.0880114808678627	lr: 0.00010
34	100	loss:	 0.10863538831472397	lr: 0.00010
34	150	loss:	 0.1009712889790535	lr: 0.00010
34	200	loss:	 0.13064761459827423	lr: 0.00010
34	250	loss:	 0.10669390112161636	lr: 0.00010
34	300	loss:	 0.07252231240272522	lr: 0.00010
34	350	loss:	 0.06271512806415558	lr: 0.00010
epoch: 34	train acc: 0.96856
epoch: 34	test acc: 0.9085